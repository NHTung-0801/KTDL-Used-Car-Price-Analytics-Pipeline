import requests
import pandas as pd
import time
import random
import os
import json
from datetime import datetime
from bs4 import BeautifulSoup

TARGET_ROWS = 10000
BASE_URL = "https://www.chotot.com/mua-ban-oto"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "vi-VN,vi;q=0.9",
}

def extract_next_data(html: str):
    soup = BeautifulSoup(html, "html.parser")
    script = soup.find("script", id="__NEXT_DATA__")
    if not script or not script.string:
        return None
    return json.loads(script.string)

def find_ads_anywhere(obj):
    """
    QuÃ©t Ä‘á»‡ quy JSON Ä‘á»ƒ tÃ¬m list ads
    Ads há»£p lá»‡: list[dict] cÃ³ 'subject' vÃ  'price'
    """
    results = []

    if isinstance(obj, dict):
        for k, v in obj.items():
            if k == "ads" and isinstance(v, list):
                if v and isinstance(v[0], dict) and "subject" in v[0]:
                    results.extend(v)
            else:
                results.extend(find_ads_anywhere(v))

    elif isinstance(obj, list):
        for item in obj:
            results.extend(find_ads_anywhere(item))

    return results


def crawl_chotot_html():
    print(f"ðŸš€ Báº¯t Ä‘áº§u cÃ o Chotot HTML (Má»¥c tiÃªu: {TARGET_ROWS})")

    os.makedirs("data/raw", exist_ok=True)
    filename = f"data/raw/chotot_full_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"

    all_cars = []
    page = 1

    while len(all_cars) < TARGET_ROWS:
        print(f"âž¡ï¸ Page {page}")

        r = requests.get(
            f"{BASE_URL}?page={page}",
            headers=HEADERS,
            timeout=20
        )

        if r.status_code != 200:
            print(f"âš ï¸ HTTP {r.status_code}")
            break

        next_data = extract_next_data(r.text)
        if not next_data:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y __NEXT_DATA__")
            break

        ads = find_ads_anywhere(next_data)

        if not ads:
            print("âš ï¸ Trang nÃ y khÃ´ng cÃ³ ads")
            break

        print(f"   âœ… TÃ¬m tháº¥y {len(ads)} xe")

        for item in ads:
            car = {
                "title": item.get("subject", ""),
                "price_raw": str(item.get("price", "")),
                "info_raw": json.dumps(item, ensure_ascii=False),
                "url": f"https://www.chotot.com/{item.get('list_id')}.htm",
                "source": "chotot",
                "crawl_date": datetime.now().strftime("%Y-%m-%d"),
            }
            all_cars.append(car)

        if len(all_cars) % 300 == 0:
            pd.DataFrame(all_cars).to_csv(
                filename, index=False, encoding="utf-8-sig"
            )
            print(f"ðŸ’¾ Checkpoint {len(all_cars)}")

        page += 1
        time.sleep(random.uniform(1.2, 2.5))

    df = pd.DataFrame(all_cars)
    df.to_csv(filename, index=False, encoding="utf-8-sig")

    print(f"âœ… HOÃ€N Táº¤T: {len(df)} xe")
    print(f"ðŸ“ File: {filename}")
    print("ðŸ‘‰ File sáºµn sÃ ng cho cleaning.py")


if __name__ == "__main__":
    crawl_chotot_html()
