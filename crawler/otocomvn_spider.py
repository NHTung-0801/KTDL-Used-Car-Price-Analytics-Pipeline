import sys
import os
# Fix Ä‘Æ°á»ng dáº«n import
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
sys.path.append(root_dir)

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from datetime import datetime
from crawler.utils import get_header

def crawl_otocomvn_brute_force(target_rows=5000):
    base_url = "https://oto.com.vn/mua-ban-xe"
    all_cars = []
    
    output_dir = os.path.join(root_dir, 'data', 'raw')
    os.makedirs("data/raw", exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    filename = os.path.join(output_dir, f"otocomvn_full_{timestamp}.csv")

    
    print(f"ğŸš€ Báº¯t Ä‘áº§u cháº¿ Ä‘á»™ 'VÆ  VÃ‰T Táº¤T Cáº¢' (Láº¥y toÃ n bá»™ text hiá»ƒn thá»‹)...")
    print(f"ğŸ’¾ File sáº½ lÆ°u táº¡i: {filename}")
    
    current_page = 1
    total_scraped = 0
    consecutive_errors = 0
    
    while total_scraped < target_rows:
        url = f"{base_url}/p{current_page}"
        print(f"   -> Äang cÃ o Trang {current_page} (ÄÃ£ cÃ³: {total_scraped} xe)")
        
        try:
            response = requests.get(url, headers=get_header(), timeout=15)
            if response.status_code != 200:
                print(f"   âš ï¸ Lá»—i káº¿t ná»‘i: {response.status_code}. Thá»­ láº¡i sau 5s...")
                time.sleep(5)
                consecutive_errors += 1
                if consecutive_errors > 5: break
                continue

            soup = BeautifulSoup(response.content, 'html.parser')
            
            # 1. TÃ¬m khung chá»©a tin Ä‘Äƒng
            listings = soup.find_all('div', class_='item-car')
            if not listings:
                listings = soup.find_all('div', class_='box-listing-car')
            
            if not listings:
                print("   âš ï¸ KhÃ´ng tÃ¬m tháº¥y xe nÃ o (Háº¿t trang hoáº·c bá»‹ cháº·n).")
                break
            
            consecutive_errors = 0 
            
            for item in listings:
                car = {}
                
                # --- A. CÃC Cá»˜T CÆ  Báº¢N (Cá»‘ gáº¯ng láº¥y riÃªng cho tiá»‡n) ---
                # TiÃªu Ä‘á»
                title_tag = item.find('a', class_='title') or item.find('h3', class_='title')
                car['title'] = title_tag.text.strip() if title_tag else "Unknown"
                
                # URL
                if title_tag and title_tag.name == 'a':
                    link = title_tag.get('href')
                elif title_tag and title_tag.find('a'):
                    link = title_tag.find('a').get('href')
                else:
                    link = ""
                car['url'] = "https://oto.com.vn" + link if link and not link.startswith('http') else link

                # GiÃ¡ (Láº¥y riÃªng Ä‘á»ƒ dá»… nhÃ¬n, nhÆ°ng cÅ©ng sáº½ cÃ³ trong info_raw)
                price_tag = item.find('span', class_='price') or item.find('p', class_='price')
                car['price_raw'] = price_tag.text.strip() if price_tag else "0"
                
                # --- B. INFO_RAW: Láº¤Y Háº¾T Má»ŒI THá»¨ (BRUTE FORCE) ---
                # Lá»‡nh get_text(separator=' | ') sáº½ láº¥y toÃ n bá»™ chá»¯ trong tháº» div,
                # bao gá»“m cáº£ nÄƒm, mÃ´ táº£, Ä‘á»‹a Ä‘iá»ƒm, ngÆ°á»i bÃ¡n, icon... 
                # cÃ¡ch nhau báº±ng dáº¥u gáº¡ch Ä‘á»©ng " | "
                full_text = item.get_text(separator=' | ', strip=True)
                
                # Loáº¡i bá» cÃ¡c kÃ½ tá»± xuá»‘ng dÃ²ng thá»«a thÃ£i
                clean_full_text = " ".join(full_text.split())
                
                car['info_raw'] = clean_full_text
                # ----------------------------------------------------

                car['source'] = 'otocomvn'
                car['crawl_date'] = datetime.now().strftime("%Y-%m-%d")
                
                all_cars.append(car)
                total_scraped += 1
                
                if total_scraped >= target_rows: break
            
            # LÆ°u Checkpoint
            if all_cars and (current_page % 5 == 0 or total_scraped >= target_rows):
                df = pd.DataFrame(all_cars)
                df.to_csv(filename, index=False, encoding='utf-8-sig')
                print(f"   ğŸ’¾ [Checkpoint] ÄÃ£ lÆ°u {len(df)} dÃ²ng.")

            current_page += 1
            time.sleep(random.uniform(2, 4))
            
        except Exception as e:
            print(f"âŒ Lá»—i trang {current_page}: {e}")
            consecutive_errors += 1
            time.sleep(5)

    print(f"âœ… Xong! File má»›i táº¡i: {filename}")
    print("ğŸ‘‰ File nÃ y cháº¯c cháº¯n cá»™t info_raw sáº½ Ä‘áº§y áº¯p chá»¯. Báº¡n hÃ£y cháº¡y láº¡i Cleaning Ä‘á»ƒ lá»c sau.")

if __name__ == "__main__":
    crawl_otocomvn_brute_force(10000)