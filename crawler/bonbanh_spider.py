import sys
import os
# Fix Ä‘Æ°á»ng dáº«n import
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
sys.path.append(root_dir)

import requests
from bs4 import BeautifulSoup
import pandas as pd 
import time
import random
from datetime import datetime
from crawler.utils import get_header
from tqdm import tqdm

def crawl_bonbanh_brute_force(target_rows=10000):
    """
    Crawl dá»¯ liá»‡u tá»« bonbanh.com vá»›i target 10,000 xe
    
    Args:
        target_rows: Sá»‘ lÆ°á»£ng xe cáº§n crawl (máº·c Ä‘á»‹nh 10000)
    """
    # Bonbanh URL format: https://bonbanh.com/oto/page,2
    base_url = "https://bonbanh.com/oto"
    all_cars = []
<<<<<<< HEAD
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    filename = f"data/raw/bonbanh_full_{timestamp}.csv"
    os.makedirs("data/raw", exist_ok=True)
=======

    output_dir = os.path.join(root_dir, 'data', 'raw')
    os.makedirs(output_dir, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    filename = os.path.join(output_dir, f"bonbanh_full_{timestamp}.csv")
>>>>>>> daca89c9e2d6901ba83017287808cf9dcda97f35
    
    print(f"ğŸš€ [BONBANH] Báº¯t Ä‘áº§u cÃ o dá»¯ liá»‡u thÃ´ (Target: {target_rows:,} xe)...")
    print(f"   (LÆ°u Ã½: Bonbanh cháº·n bot ráº¥t gáº¯t, code sáº½ cháº¡y cháº­m Ä‘á»ƒ an toÃ n)")
    print(f"   ğŸ“ File sáº½ Ä‘Æ°á»£c lÆ°u táº¡i: {filename}\n")
    
    current_page = 1
    total_scraped = 0
    consecutive_errors = 0
    max_consecutive_errors = 5
    empty_pages_count = 0
    max_empty_pages = 3
    
    # Progress bar
    pbar = tqdm(total=target_rows, desc="Äang crawl", unit="xe", ncols=100)
    
    while total_scraped < target_rows:
        # Cáº¥u trÃºc link page cá»§a Bonbanh
        if current_page == 1:
            url = base_url
        else:
            url = f"{base_url}/page,{current_page}"
            
        try:
            # ThÃªm delay ngáº«u nhiÃªn Ä‘á»ƒ trÃ¡nh bá»‹ cháº·n
            delay = random.uniform(3, 7)
            time.sleep(delay)
            
            response = requests.get(url, headers=get_header(), timeout=20)
            
            if response.status_code != 200:
                print(f"\n   âš ï¸ Lá»—i káº¿t ná»‘i {response.status_code} táº¡i trang {current_page}. Nghá»‰ 15s rá»“i thá»­ láº¡i...")
                time.sleep(15)
                consecutive_errors += 1
                if consecutive_errors >= max_consecutive_errors:
                    print(f"\n   âŒ ÄÃ£ cÃ³ {max_consecutive_errors} lá»—i liÃªn tiáº¿p. Dá»«ng crawler.")
                    break
                continue
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # TÃ¬m danh sÃ¡ch xe vá»›i nhiá»u selector dá»± phÃ²ng
            listings = []
            
            # Thá»­ nhiá»u cÃ¡ch tÃ¬m listings
<<<<<<< HEAD
            listings = soup.find_all('li', class_='car-item')
            if not listings:
                listings = soup.find_all('div', class_='car-item')
=======
            listings = soup.find_all('li', class_='car-detail')
            if not listings:
                listings = soup.find_all('div', class_='car-detail')
>>>>>>> daca89c9e2d6901ba83017287808cf9dcda97f35
            if not listings:
                listings = soup.find_all('li', class_=lambda x: x and 'car' in x.lower())
            if not listings:
                listings = soup.find_all('div', class_=lambda x: x and 'car' in x.lower())
            if not listings:
                # Thá»­ tÃ¬m trong cÃ¡c container phá»• biáº¿n
                containers = soup.find_all(['ul', 'div'], class_=lambda x: x and ('list' in x.lower() or 'grid' in x.lower()))
                for container in containers:
                    listings = container.find_all(['li', 'div'], recursive=False)
                    if listings:
                        break
            
            if not listings:
                empty_pages_count += 1
                print(f"\n   âš ï¸ Trang {current_page}: KhÃ´ng tÃ¬m tháº¥y xe nÃ o (CÃ³ thá»ƒ bá»‹ cháº·n hoáº·c háº¿t trang).")
                if empty_pages_count >= max_empty_pages:
                    print(f"   âŒ ÄÃ£ cÃ³ {max_empty_pages} trang trá»‘ng liÃªn tiáº¿p. Dá»«ng crawler.")
                break
                current_page += 1
                continue
            
            # Reset counters khi tÃ¬m tháº¥y dá»¯ liá»‡u
            consecutive_errors = 0
            empty_pages_count = 0
            
            page_cars_count = 0
            
            for item in listings:
                if total_scraped >= target_rows:
                    break
                    
                car = {}
                
                # --- 1. TIÃŠU Äá»€ & URL ---
                # Thá»­ nhiá»u cÃ¡ch tÃ¬m title
                title_tag = None
                title_link = None
                
                # CÃ¡ch 1: h3 > a
                title_tag = item.find('h3')
                if title_tag:
                    title_link = title_tag.find('a')
                
                # CÃ¡ch 2: a vá»›i class title
                if not title_link:
                    title_link = item.find('a', class_=lambda x: x and 'title' in x.lower())
                
                # CÃ¡ch 3: a Ä‘áº§u tiÃªn
                if not title_link:
                    title_link = item.find('a')
                
                if title_link:
                    car['title'] = title_link.text.strip()
                    url_suffix = title_link.get('href', '')
                    if url_suffix:
                        if url_suffix.startswith('http'):
                            car['url'] = url_suffix
                        elif url_suffix.startswith('/'):
                            car['url'] = "https://bonbanh.com" + url_suffix
                        else:
                            car['url'] = "https://bonbanh.com/" + url_suffix
                    else:
                        car['url'] = url
                else:
                    # Dá»± phÃ²ng: láº¥y text Ä‘áº§u tiÃªn lÃ m title
                    text_parts = item.get_text(strip=True).split('\n')
                    car['title'] = text_parts[0][:100] if text_parts else "Unknown"
                    car['url'] = url

                # --- 2. GIÃ ---
                # Thá»­ nhiá»u cÃ¡ch tÃ¬m giÃ¡
                price_text = "0"
                
                # CÃ¡ch 1: div class cb3
                price_tag = item.find('div', class_='cb3')
                if price_tag:
                    price_text = price_tag.text.strip()
                else:
                    # CÃ¡ch 2: b vá»›i itemprop='price'
                    price_b = item.find('b', itemprop='price')
                    if price_b:
                        price_text = price_b.text.strip()
                    else:
                        # CÃ¡ch 3: span/div vá»›i class chá»©a 'price'
                        price_elem = item.find(['span', 'div', 'p'], class_=lambda x: x and 'price' in x.lower())
                        if price_elem:
                            price_text = price_elem.text.strip()
                        else:
                            # CÃ¡ch 4: tÃ¬m text chá»©a "tá»·", "triá»‡u", "VNÄ"
                            all_text = item.get_text()
                            if any(keyword in all_text for keyword in ['tá»·', 'triá»‡u', 'VNÄ', 'Ä‘á»“ng']):
                                # Láº¥y Ä‘oáº¡n text cÃ³ chá»©a giÃ¡
                                lines = all_text.split('\n')
                                for line in lines:
                                    if any(keyword in line for keyword in ['tá»·', 'triá»‡u', 'VNÄ']):
                                        price_text = line.strip()
                                        break
                
                car['price_raw'] = price_text
                
                # --- 3. Láº¤Y TOÃ€N Bá»˜ INFO (BRUTE FORCE) ---
                # Láº¥y háº¿t text trong item, ngÄƒn cÃ¡ch báº±ng dáº¥u |
                full_text = item.get_text(separator=' | ', strip=True)
                
                # LÃ m sáº¡ch bá»›t xuá»‘ng dÃ²ng thá»«a
                clean_full_text = " ".join(full_text.split())
                
                # LÆ°u vÃ o info_raw Ä‘á»ƒ cleaning.py xá»­ lÃ½
                car['info_raw'] = clean_full_text
                
                # Metadata
                car['source'] = 'bonbanh'
                car['crawl_date'] = datetime.now().strftime("%Y-%m-%d")
                
                all_cars.append(car)
                total_scraped += 1
                page_cars_count += 1
                pbar.update(1)
            
            # LÆ°u checkpoint sau má»—i trang Ä‘á»ƒ trÃ¡nh máº¥t dá»¯ liá»‡u
            if all_cars:
                try:
                    df = pd.DataFrame(all_cars)
                    df.to_csv(filename, index=False, encoding='utf-8-sig')
                except Exception as save_error:
                    print(f"\n   âš ï¸ Lá»—i khi lÆ°u file: {save_error}")
            
            if page_cars_count > 0:
                pbar.set_postfix({
                    'Trang': current_page,
                    'Xe/trang': page_cars_count,
                    'Tá»•ng': total_scraped
                })

            current_page += 1
            
        except requests.exceptions.Timeout:
            print(f"\n   âš ï¸ Timeout táº¡i trang {current_page}. Nghá»‰ 10s rá»“i thá»­ láº¡i...")
            consecutive_errors += 1
            time.sleep(10)
            if consecutive_errors >= max_consecutive_errors:
                break
                
        except requests.exceptions.RequestException as e:
            print(f"\n   âš ï¸ Lá»—i káº¿t ná»‘i táº¡i trang {current_page}: {e}. Nghá»‰ 15s rá»“i thá»­ láº¡i...")
            consecutive_errors += 1
            time.sleep(15)
            if consecutive_errors >= max_consecutive_errors:
                break
            
        except Exception as e:
            print(f"\n   âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh táº¡i trang {current_page}: {e}")
            consecutive_errors += 1
            time.sleep(10)
            if consecutive_errors >= max_consecutive_errors:
                break
    
    pbar.close()
    
    # LÆ°u file cuá»‘i cÃ¹ng
    if all_cars:
        try:
            df = pd.DataFrame(all_cars)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"\nâœ… HoÃ n thÃ nh! ÄÃ£ crawl Ä‘Æ°á»£c {len(all_cars):,} xe")
            print(f"ğŸ“ File Ä‘Ã£ lÆ°u táº¡i: {filename}")
        except Exception as e:
            print(f"\nâŒ Lá»—i khi lÆ°u file cuá»‘i cÃ¹ng: {e}")
    else:
        print(f"\nâš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u nÃ o Ä‘Æ°á»£c crawl!")
    
    print(f"\nğŸ‘‰ BÃ¢y giá» báº¡n hÃ£y cháº¡y 'python preprocessing/cleaning.py' Ä‘á»ƒ lÃ m sáº¡ch dá»¯ liá»‡u.")

if __name__ == "__main__":
    # Crawl 10,000 xe
<<<<<<< HEAD
    crawl_bonbanh_brute_force(10000)
=======
    crawl_bonbanh_brute_force(100)
>>>>>>> daca89c9e2d6901ba83017287808cf9dcda97f35
