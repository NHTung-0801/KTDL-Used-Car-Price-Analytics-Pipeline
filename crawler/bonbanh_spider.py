import sys
import os
# Fix ƒë∆∞·ªùng d·∫´n import
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
sys.path.append(root_dir)

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from datetime import datetime
from crawler.utils import get_header

def crawl_bonbanh_brute_force(target_rows=100):
    # Bonbanh URL format: https://bonbanh.com/oto/page,2
    base_url = "https://bonbanh.com/oto"
    all_cars = []
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    filename = f"data/raw/bonbanh_full_{timestamp}.csv"
    os.makedirs("data/raw", exist_ok=True)
    
    print(f"üöÄ [BONBANH] B·∫Øt ƒë·∫ßu c√†o d·ªØ li·ªáu th√¥ (Target: {target_rows} xe)...")
    print(f"   (L∆∞u √Ω: Bonbanh ch·∫∑n bot r·∫•t g·∫Øt, code s·∫Ω ch·∫°y ch·∫≠m ƒë·ªÉ an to√†n)")
    
    current_page = 1
    total_scraped = 0
    consecutive_errors = 0
    
    while total_scraped < target_rows:
        # C·∫•u tr√∫c link page c·ªßa Bonbanh
        if current_page == 1:
            url = base_url
        else:
            url = f"{base_url}/page,{current_page}"
            
        print(f"   -> ƒêang c√†o Trang {current_page} (ƒê√£ c√≥: {total_scraped} xe)")
        
        try:
            response = requests.get(url, headers=get_header(), timeout=15)
            
            if response.status_code != 200:
                print(f"   ‚ö†Ô∏è L·ªói k·∫øt n·ªëi {response.status_code}. Ngh·ªâ 10s r·ªìi th·ª≠ l·∫°i...")
                time.sleep(10)
                consecutive_errors += 1
                if consecutive_errors > 3: break
                continue
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # T√¨m danh s√°ch xe (Bonbanh th∆∞·ªùng d√πng ul > li.car-item)
            listings = soup.find_all('li', class_='car-item')
            
            if not listings:
                print("   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y xe n√†o (C√≥ th·ªÉ b·ªã ch·∫∑n ho·∫∑c h·∫øt trang).")
                break
            
            consecutive_errors = 0
            
            for item in listings:
                car = {}
                
                # --- 1. TI√äU ƒê·ªÄ & URL ---
                # Th∆∞·ªùng n·∫±m trong h3 > a
                title_tag = item.find('h3')
                if title_tag and title_tag.find('a'):
                    title_link = title_tag.find('a')
                    car['title'] = title_link.text.strip()
                    url_suffix = title_link.get('href')
                    car['url'] = "https://bonbanh.com/" + url_suffix if not url_suffix.startswith('http') else url_suffix
                else:
                    # D·ª± ph√≤ng n·∫øu c·∫•u tr√∫c kh√°c
                    car['title'] = item.get_text().split('\n')[0][:50]
                    car['url'] = url

                # --- 2. GI√Å ---
                # Bonbanh th∆∞·ªùng ƒë·ªÉ gi√° trong tag <b> ho·∫∑c class cb3
                price_tag = item.find('div', class_='cb3')
                if price_tag:
                    car['price_raw'] = price_tag.text.strip()
                else:
                    # T√¨m th·∫ª b ch·ª©a gi√°
                    price_b = item.find('b', itemprop='price')
                    car['price_raw'] = price_b.text.strip() if price_b else "0"
                
                # --- 3. L·∫§Y TO√ÄN B·ªò INFO (BRUTE FORCE) ---
                # L·∫•y h·∫øt text trong th·∫ª li, ngƒÉn c√°ch b·∫±ng d·∫•u |
                full_text = item.get_text(separator=' | ', strip=True)
                
                # L√†m s·∫°ch b·ªõt xu·ªëng d√≤ng th·ª´a
                clean_full_text = " ".join(full_text.split())
                
                # L∆∞u v√†o info_raw ƒë·ªÉ cleaning.py x·ª≠ l√Ω
                car['info_raw'] = clean_full_text
                
                # Metadata
                car['source'] = 'bonbanh'
                car['crawl_date'] = datetime.now().strftime("%Y-%m-%d")
                
                all_cars.append(car)
                total_scraped += 1
                
                if total_scraped >= target_rows: break
            
            # L∆∞u checkpoint
            if all_cars:
                pd.DataFrame(all_cars).to_csv(filename, index=False, encoding='utf-8-sig')

            current_page += 1
            # Bonbanh r·∫•t nh·∫°y c·∫£m, n√™n ngh·ªâ l√¢u h∆°n (3-6 gi√¢y)
            time.sleep(random.uniform(3, 6))
            
        except Exception as e:
            print(f"‚ùå L·ªói trang {current_page}: {e}")
            consecutive_errors += 1
            time.sleep(5)

    print(f"\n‚úÖ Xong Bonbanh! File raw: {filename}")
    print("üëâ B√¢y gi·ªù b·∫°n h√£y ch·∫°y 'python preprocessing/cleaning.py' ƒë·ªÉ xem n√≥ c√≥ l·ªçc ƒë∆∞·ª£c kh√¥ng.")

if __name__ == "__main__":
    # Test tr∆∞·ªõc 200 d√≤ng
    crawl_bonbanh_brute_force(200)